# 深度模型中的优化
### 目录  
+ [应用数学与数学基础](../应用数学与数学基础/ "../应用数学与数学基础/") 
    + [线性代数](../应用数学与数学基础/线性代数.md "../应用数学与数学基础/线性代数.md")
    + [概率与信息论](../应用数学与数学基础/概率与信息论.md "../应用数学与数学基础/概率与信息论.md") 
    + [数值计算](../应用数学与数学基础/数值计算.md "../应用数学与数学基础/数值计算.md")
    + [机器学习基础](../应用数学与数学基础/机器学习基础.md "../应用数学与数学基础/机器学习基础.md")
+ [深度网络:现代实践](../深度网络：现代实践/ "../深度网络：现代实践/")
    + [深度前馈网络](../深度网络：现代实践/深度前馈网络.md "../深度网络：现代实践/深度前馈网络.md")
    + [深度学习中的正则化](../深度网络：现代实践/深度学习中的正则化.md "../深度网络：现代实践/深度学习中的正则化.md")
    + [**深度模型中的优化**](../深度网络：现代实践/深度模型中的优化.md "../深度网络：现代实践/深度模型中的优化.md")
    + [卷积网络](../深度网络：现代实践/卷积网络.md "../深度网络：现代实践/卷积网络.md")
    + [序列建模:循环和递归网络](../深度网络：现代实践/序列建模：循环和递归网络.md "../深度网络：现代实践/序列建模：循环和递归网络.md")
    + [实践方法论](../深度网络：现代实践/实践方法论.md "../深度网络：现代实践/实践方法论.md")
    + [应用](../深度网络：现代实践/应用.md "../深度网络：现代实践/应用.md")
+ [深度学习研究](../深度学习研究/ "../深度学习研究/")
    + [线性因子模型](../深度学习研究/线性因子模型.md "../深度学习研究/线性因子模型.md")
    + [自编码器](../深度学习研究/自编码器.md "../深度学习研究/自编码器.md")
    + [表示学习](../深度学习研究/表示学习.md "../深度学习研究/表示学习.md")
    + [深度学习中的结构化概率模型](../深度学习研究/深度学习中的结构化概率模型.md "../深度学习研究/深度学习中的结构化概率模型.md")
    + [蒙特卡罗方法](../深度学习研究/蒙特卡罗方法.md "../深度学习研究/蒙特卡罗方法.md")
    + [直面配分函数](../深度学习研究/直面配分函数.md "../深度学习研究/直面配分函数.md")
    + [近似推断](../深度学习研究/近似推断.md "../深度学习研究/近似推断.md")
    + [深度生成模型](../深度学习研究/深度生成模型.md "../深度学习研究/深度生成模型.md")

## 前言
　　本文主要介绍神经网络训练中的优化技术，兵主要关注一类特定的优化问题：寻找神经网络上的一组参数θ，它能显著地降低代价函数J(θ)，
该代价函数通常包括整个训练集上的性能评估和额外的正则化项。  
　　本文首先会介绍机器学习任务中作为训练算法使用的优化与纯优化有哪些不同；其次，会介绍导致神经网络优化困难的几个具体挑战；然后，
会介绍几个使用算法，包括优化算法本身和初始化参数的策略，包括更高级的可以自适应调整学习率和使用二阶导数的信息的算法；最后，
将介绍几个简单优化算法结合成高级过程的优化策略。
## 深度模型训练的优化算法和传统优化算法的不同
　　在大多数的机器学习问题上，我们只关注某些性能度量P，P定义于测试集上并且可能是不可解的，这导致我们只能间接优化P。而纯优化是最小化目标J本身。
通常来讲，代价函数可写为训练集上的平均，更进一步的是希望最小化取自数据生成分布p<sub>data</sub>的期望，其公式如下所示　　
<div align=center name="(1-1)"><img src="https://latex.codecogs.com/gif.latex?J^{*}(\theta)=\mathbb{E}_{\left&space;(&space;x,y&space;\right&space;)\sim&space;p_{data}}L\left&space;(&space;f\left&space;(&space;x;\theta&space;\right&space;),&space;y&space;\right&space;)" title="J^{*}(\theta)=\mathbb{E}_{\left ( x,y \right )\sim p_{data}}L\left ( f\left ( x;\theta \right ), y \right )" /><div align=right>(1-1)</div></div>

### 经验风险最小化
　　机器算法的目标是降低[代价函数](#(1-1))的期望泛化误差，这样的误差我们称之为风险，而通常我们是无法知道p<sub>data</sub>(x,y)的，
只能知道训练集中的样本。而将机器学习问题转化会一个优化问题的最简单方法是最小化训练集上的期望损失，这意味着我们必须用训练集上的经验分布
<a><img src="https://latex.codecogs.com/gif.latex?\widehat{p}\left(&space;x,y\right)" title="\widehat{p}\left( x,y\right)" /></a>
替代真实分布p(x,y)，进而最小化经验风险：  
<div align=center name="(1-2)"><img src="https://latex.codecogs.com/gif.latex?\mathbb{E}_{x,y\sim&space;\widehat{p}_{data}}\left&space;[&space;L&space;\left&space;(&space;f\left&space;(&space;x;\theta&space;\right&space;),&space;y&space;\right&space;)\right&space;]=\frac{1}{m}\sum_{i=1}^{m}L\left&space;(&space;f\left&space;(&space;x^{i};\theta&space;\right&space;),&space;y^{i})&space;\right&space;)" title="\mathbb{E}_{x,y\sim \widehat{p}_{data}}\left [ L \left ( f\left ( x;\theta \right ), y \right )\right ]=\frac{1}{m}\sum_{i=1}^{m}L\left ( f\left ( x^{i};\theta \right ), y^{i}) \right )" /><div align=right>(1-2)</div></div>  
但是经验风险最小化容易导致过拟合，此外，很多情况下，经验风险最小化并不一定可行的，比如对于0-1损失函数根据经验风险最小化是没有有效导数的。

### 代理损失函数和提前终止
　　正如上面所说的，有时候真正的损失函数并不能被高效优化，这种情况下，我们通常会去优化其代理损失函数，比如0-1损失函数，我们不能直接有效优化，
但是可以通过正确列别的负对数似然函数来代替0-1损失函数。在某些情况下，代理损失函数要比原损失函数学到的更多。一般的优化和用于训练算法的优化的
一个重要的不同是训练算法通常不会停止在局部极小点，但是在基于提前终止([深度学习中的正则化-提前终止](./深度学习中的正则化.md#提前终止))的收敛条件满足时停止。
与纯优化不同的是，提前终止时代理损失函数的仍然有较大的导数，而纯优化终止时导数较小。

### 批量算法和小批量算法
　　机器学习算法和一般优化算法不同的一点是，机器学习算法的目标函数通常可以分解为训练样本上的求和，因此，可以使用整个训练集的优化算法，
被称为批量或确定性的梯度算法，而每次使用单个样本的优化算法被称为随机或者在线算法，大多数的深度学习算法则是介于上述两者之间，
即使用一个以上而又不是全部的训练样本，称之为小批量或者小批量随机(常常简称为随机方法)，小批量的大小通常由以下几个因素决定的：  

+ 更大的批量会计算更精确的梯度估计，但是回报却是小于线性的。
+ 极小批量通常难以充分利用多核架构。这促使我们使用一些绝对最小批量，低于这个值的最小批量处理不会减少计算时间。
+ 如果批量处理中的所有样本可以并行处理，那么内存消耗和批量大小会正比。对于很多硬件设施，这是批量大小的限制因素。
+ 在某些硬件上使用特定大小的数组时，运行时间会更少。尤其是在使用GPU时，通常会使用2的幂数作为批量大小可以获得更少的运行时间。
+ 可能是由于小批量在学习过程中加入噪声，会有一些正则化效果<sup>[\[1\]](#footnote1)。  
  
　　不同的算法使用不同的方法从小批量中获取不同的信息；小批量是随机抽取的这点也很重要；很多机器学习上的优化问题都可以分解成并行地计算不同样本上单独的更新。  
　　小批量随机梯度下降一个有趣的现象是只要没有重复使用样本，那么它将遵循着真实泛化误差的梯度，即第一次遍历数据时，每个小批量样本计算出来的
真实泛化误差都是无偏估计，当重复使用后得到的真实泛化误差估计是有偏的。因为学习器每次看到的样本(x,y)都是来自数据生成分布p<sub>data</sub>(x,y)，
这种情况意味着样本永远不会重复，获得的样本都是无偏样本。
## 神经网络优化中的挑战
　　优化是一个及其困难的任务，传统的机器学习都会精心设计目标函数和约束，以确保优化问题是凸的；然而，我们在训练神经网络时，都会碰到非凸问题，
即便是凸优化问题，也并非能够在优化上一帆风顺的。
### 病态
　　病态体现在随机梯度下降会卡在某些情况，这其中最突出的是Hessian矩阵H的病态。我们假定代价损失函数的近似二阶泰勒级数展开为：  
<div align=center name="(1-3)"><img src="https://latex.codecogs.com/gif.latex?f(x)\approx&space;f(x^{(0)})&space;&plus;&space;(x&space;-&space;x^{(0)})^{\top}g&plus;\frac{1}{2}(x-x^{(0)})^{\top}H(x-x^{(0)})" title="f(x)\approx f(x^{(0)}) + (x - x^{(0)})^{\top}g+\frac{1}{2}(x-x^{(0)})^{\top}H(x-x^{(0)})" /><div align=right>(1-3)</div></div>  
　　我们假设x = x<sup>(0)</sup> - εg，那么上述公式可转化为：  
<div align=center name="(1-4)"><img src="https://latex.codecogs.com/gif.latex?f(x^{(0)}&space;-&space;\epsilon&space;g)\approx&space;f(x^{(0)})&space;-&space;\epsilon&space;g^{\top}g&plus;\frac{1}{2}\epsilon^{2}g&space;^{\top}Hg" title="f(x^{(0)} - \epsilon g)\approx f(x^{(0)}) - \epsilon g^{\top}g+\frac{1}{2}\epsilon^{2}g ^{\top}Hg" /><div align=right>(1-4)</div></div>  
　　所以，代价损失函数的二阶泰勒级数展开的预测梯度下降中的-εg会使得损失代价误差增加  
<div align=center name="(1-5)"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{2}\epsilon^{2}g&space;^{\top}Hg&space;-&space;\epsilon&space;g^{\top}g" title="\frac{1}{2}\epsilon^{2}g ^{\top}Hg - \epsilon g^{\top}g" /><div align=right>(1-5)</div></div>   
　　当<a><img src="https://latex.codecogs.com/gif.latex?\inline&space;\small&space;\frac{1}{2}\epsilon^{2}g^{\top}Hg" title="\small \frac{1}{2}\epsilon^{2}g^{\top}Hg" /></a>超过<a><img src="https://latex.codecogs.com/gif.latex?\inline&space;\small&space;\epsilon&space;g^{\top}g"/></a>时，
梯度的病态就会成为问题。因此，判断病态是否出现在神经网络训练任务中，可以通过检测平方梯度的范数g<sup>T</sup>g和g<sup>T</sup>Hg来看，
尽管有如牛顿法可以解决病态的技术，但并不是适应神经网络的训练。

<a name="footnote1">\[1\]</a> [Wilson, D. R. , & Martinez, T. R. . (2003). The general inefficiency of batch training for gradient descent learning. Neural Networks, 16(10), 1429-1451.](http://xueshu.baidu.com/usercenter/paper/show?paperid=0fc821159b2985ccd03c0f28a611aadc&site=xueshu_se)



