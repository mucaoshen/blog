# 深度模型中的优化
### 目录  
+ [应用数学与数学基础](../应用数学与数学基础/ "../应用数学与数学基础/") 
    + [线性代数](../应用数学与数学基础/线性代数.md "../应用数学与数学基础/线性代数.md")
    + [概率与信息论](../应用数学与数学基础/概率与信息论.md "../应用数学与数学基础/概率与信息论.md") 
    + [数值计算](../应用数学与数学基础/数值计算.md "../应用数学与数学基础/数值计算.md")
    + [机器学习基础](../应用数学与数学基础/机器学习基础.md "../应用数学与数学基础/机器学习基础.md")
+ [深度网络:现代实践](../深度网络：现代实践/ "../深度网络：现代实践/")
    + [深度前馈网络](../深度网络：现代实践/深度前馈网络.md "../深度网络：现代实践/深度前馈网络.md")
    + [深度学习中的正则化](../深度网络：现代实践/深度学习中的正则化.md "../深度网络：现代实践/深度学习中的正则化.md")
    + [**深度模型中的优化**](../深度网络：现代实践/深度模型中的优化.md "../深度网络：现代实践/深度模型中的优化.md")
    + [卷积网络](../深度网络：现代实践/卷积网络.md "../深度网络：现代实践/卷积网络.md")
    + [序列建模:循环和递归网络](../深度网络：现代实践/序列建模：循环和递归网络.md "../深度网络：现代实践/序列建模：循环和递归网络.md")
    + [实践方法论](../深度网络：现代实践/实践方法论.md "../深度网络：现代实践/实践方法论.md")
    + [应用](../深度网络：现代实践/应用.md "../深度网络：现代实践/应用.md")
+ [深度学习研究](../深度学习研究/ "../深度学习研究/")
    + [线性因子模型](../深度学习研究/线性因子模型.md "../深度学习研究/线性因子模型.md")
    + [自编码器](../深度学习研究/自编码器.md "../深度学习研究/自编码器.md")
    + [表示学习](../深度学习研究/表示学习.md "../深度学习研究/表示学习.md")
    + [深度学习中的结构化概率模型](../深度学习研究/深度学习中的结构化概率模型.md "../深度学习研究/深度学习中的结构化概率模型.md")
    + [蒙特卡罗方法](../深度学习研究/蒙特卡罗方法.md "../深度学习研究/蒙特卡罗方法.md")
    + [直面配分函数](../深度学习研究/直面配分函数.md "../深度学习研究/直面配分函数.md")
    + [近似推断](../深度学习研究/近似推断.md "../深度学习研究/近似推断.md")
    + [深度生成模型](../深度学习研究/深度生成模型.md "../深度学习研究/深度生成模型.md")

## 前言
　　本文主要介绍神经网络训练中的优化技术，兵主要关注一类特定的优化问题：寻找神经网络上的一组参数θ，它能显著地降低代价函数J(θ)，
该代价函数通常包括整个训练集上的性能评估和额外的正则化项。  
　　本文首先会介绍机器学习任务中作为训练算法使用的优化与纯优化有哪些不同；其次，会介绍导致神经网络优化困难的几个具体挑战；然后，
会介绍几个使用算法，包括优化算法本身和初始化参数的策略，包括更高级的可以自适应调整学习率和使用二阶导数的信息的算法；最后，
将介绍几个简单优化算法结合成高级过程的优化策略。
## 深度模型训练的优化算法和传统优化算法的不同
　　在大多数的机器学习问题上，我们只关注某些性能度量P，P定义于测试集上并且可能是不可解的，这导致我们只能间接优化P。而纯优化是最小化目标J本身。
通常来讲，代价函数可写为训练集上的平均，更进一步的是希望最小化取自数据生成分布p<sub>data</sub>的期望，其公式如下所示　　
<div align=center text-align=right name="目标函数"><img src="https://latex.codecogs.com/gif.latex?J^{*}(\theta)=\mathbb{E}_{\left&space;(&space;x,y&space;\right&space;)\sim&space;p_{data}}L\left&space;(&space;f\left&space;(&space;x;\theta&space;\right&space;),&space;y&space;\right&space;)" title="J^{*}(\theta)=\mathbb{E}_{\left ( x,y \right )\sim p_{data}}L\left ( f\left ( x;\theta \right ), y \right )" />(1-1)</div>

### 经验风险最小化
　　机器算法的目标是降低[代价函数](#目标函数)的期望泛化误差，这样的误差我们称之为风险，而通常我们是无法知道p<sub>data</sub>(x,y)的，
只能知道训练集中的样本
### 第三方
###第三方
### 第三方
###第三方
### 第三方
###第三方
### 第三方
###第三方
### 第三方
###第三方
### 第三方
###第三方
### 第三方
###第三方### 第三方
###第三方### 第三方
###第三方
### 第三方
###第三方
[代价函数](#目标函数)



